{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Import Packages"
      ],
      "metadata": {
        "id": "pMbRZTn2W8sX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import f1_score, label_ranking_average_precision_score, label_ranking_loss, roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import ast"
      ],
      "metadata": {
        "id": "YSC4Ji-c6SRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the dataset"
      ],
      "metadata": {
        "id": "nmGMMFbuW-yv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "df = pd.read_excel(\"./dataset/train_data.xlsx\", engine='openpyxl')\n",
        "test_df = pd.read_excel(\"./dataset/test_data.xlsx\", engine='openpyxl')"
      ],
      "metadata": {
        "id": "vWPn2ot06TM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Pre-processing"
      ],
      "metadata": {
        "id": "IU_CsgT2XAkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "climate_features = [\n",
        "    'tavg_2020_mean', 'prec_2020_mean',\n",
        "    'tavg_2021_mean', 'prec_2021_mean',\n",
        "    'tavg_2022_mean', 'prec_2022_mean',\n",
        "    'tavg_2023_mean', 'prec_2023_mean',\n",
        "    'tavg_2024_mean', 'prec_2024_mean'\n",
        "]\n",
        "\n",
        "numeric_features = ['latitude', 'longitude', 'elevation']\n",
        "categorical_features = ['Land_Cover_Type']"
      ],
      "metadata": {
        "id": "yYQCA4nl6UlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform the training data\n",
        "X_climate_train = df[climate_features].fillna(0)\n",
        "scaler_climate = StandardScaler()\n",
        "X_scaled_train = scaler_climate.fit_transform(X_climate_train)\n",
        "\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "climate_pca_train = pca.fit_transform(X_scaled_train)\n",
        "\n",
        "df['climate_PC1'] = climate_pca_train[:, 0]\n",
        "df['climate_PC2'] = climate_pca_train[:, 1]\n",
        "\n",
        "print(\"Train PCA - Explained variance:\", pca.explained_variance_ratio_)\n",
        "print(\"Train PCA - Cumulative variance:\", np.cumsum(pca.explained_variance_ratio_))"
      ],
      "metadata": {
        "id": "jE4UuGJR6XWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform test data using previously fitted scaler and PCA\n",
        "X_climate_test = test_df[climate_features].fillna(0)\n",
        "X_scaled_test = scaler_climate.transform(X_climate_test)\n",
        "climate_pca_test = pca.transform(X_scaled_test)\n",
        "\n",
        "test_df['climate_PC1'] = climate_pca_test[:, 0]\n",
        "test_df['climate_PC2'] = climate_pca_test[:, 1]\n",
        "\n",
        "df.drop(columns=climate_features, inplace=True, errors='ignore')\n",
        "test_df.drop(columns=climate_features, inplace=True, errors='ignore')"
      ],
      "metadata": {
        "id": "mpMLflpX6Y2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocess the data\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numeric_features + ['climate_PC1', 'climate_PC2']),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "    ],\n",
        "    remainder='drop'\n",
        ")\n",
        "\n",
        "# Fit on training data\n",
        "X_processed = preprocessor.fit_transform(df)\n",
        "print(\"Train processed shape:\", X_processed.shape)\n",
        "\n",
        "# Transform test data\n",
        "X_test_processed = preprocessor.transform(test_df)\n",
        "print(\"Test processed shape:\", X_test_processed.shape)\n"
      ],
      "metadata": {
        "id": "r5HCABbJ6amp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Target labels\n",
        "y = df['train_ids']\n",
        "y_test = test_df['species']\n",
        "\n",
        "# Training\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "\n",
        "# Test (multilabel)\n",
        "y_test_lists = [ast.literal_eval(str(species_str)) if isinstance(species_str, str)\n",
        "                else species_str for species_str in y_test]\n",
        "y_test_enc = [le.transform(species_list) for species_list in y_test_lists]\n",
        "\n",
        "# Convert the test to binary matrix\n",
        "mlb = MultiLabelBinarizer(classes=range(len(le.classes_)))\n",
        "y_test_multilabel = mlb.fit_transform(y_test_enc)\n",
        "\n",
        "print(f\"Test multilabel shape: {y_test_multilabel.shape}\")\n",
        "print(f\"Avg species per test sample: {y_test_multilabel.sum(axis=1).mean():.2f}\")"
      ],
      "metadata": {
        "id": "4TA4aMwl6cI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_encoded.shape"
      ],
      "metadata": {
        "id": "R_9RqVzhEfPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_multilabel.shape"
      ],
      "metadata": {
        "id": "c30OV7N3Eh5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split( X_processed, y_encoded, test_size=0.1, random_state=42, stratify=y_encoded )\n",
        "\n",
        "print(f\"Train: {X_train.shape}, Val: {X_val.shape}\")"
      ],
      "metadata": {
        "id": "PvJkbwrD6d5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculating weights (Upweighting)"
      ],
      "metadata": {
        "id": "hHYvZSSu07GQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_class_weights(y_train):\n",
        "    counts = pd.Series(y_train).value_counts()\n",
        "    class_weights = {cls: 1.0/np.sqrt(count) for cls, count in counts.items()}\n",
        "    return class_weights\n",
        "\n",
        "class_weights = calculate_class_weights(y_train)"
      ],
      "metadata": {
        "id": "R0ZybIpk6fJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training and Evaluation\n"
      ],
      "metadata": {
        "id": "Dq-BgsOyXEs3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import psutil\n",
        "import os\n",
        "\n",
        "def train_and_evaluate(model, model_name, X_train, y_train, class_weights=None):\n",
        "\n",
        "    print(f\"Training: {model_name}\")\n",
        "\n",
        "    process = psutil.Process(os.getpid())\n",
        "\n",
        "    # Memory before training\n",
        "    mem_before = process.memory_info().rss / 1024 / 1024  # MB\n",
        "\n",
        "    # Train\n",
        "    start = time.time()\n",
        "    if class_weights is not None and hasattr(model, 'class_weight'):\n",
        "        model.set_params(class_weight=class_weights)\n",
        "    model.fit(X_train, y_train)\n",
        "    train_time = time.time() - start\n",
        "\n",
        "    # Memory after training\n",
        "    mem_after = process.memory_info().rss / 1024 / 1024  # MB\n",
        "    mem_used = mem_after - mem_before\n",
        "\n",
        "    print(f\"Training completed in {train_time:.2f} seconds\")\n",
        "    print(f\"Memory used: {mem_used:.2f} MB (Total: {mem_after:.2f} MB)\")\n",
        "\n",
        "    return model, train_time, mem_used"
      ],
      "metadata": {
        "id": "k-4WLYC5-02G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter search"
      ],
      "metadata": {
        "id": "rUZQ2Yik_UxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_metric(df, train_col, val_col, title, y_label):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    colors = ['blue', 'green','orange']\n",
        "    split_vals = sorted(df['min_samples_split'].unique())\n",
        "\n",
        "    for i, split_val in enumerate(split_vals):\n",
        "        subset = df[df['min_samples_split'] == split_val]\n",
        "        curr_color = colors[i % len(colors)]\n",
        "\n",
        "        plt.plot(subset['max_depth'], subset[val_col],\n",
        "                 marker='o', label=f'Val (split={split_val})',\n",
        "                 color=curr_color, linestyle='-')\n",
        "\n",
        "    plt.title(title, fontsize=14)\n",
        "    plt.xlabel('Max Depth', fontsize=12)\n",
        "    plt.ylabel(y_label, fontsize=12)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.legend(loc='best')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_xgb_metric(df, train_col, val_col, title, y_label):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    colors = ['blue', 'orange']\n",
        "    est_vals = sorted(df['n_estimators'].unique())\n",
        "\n",
        "    for i, n_est in enumerate(est_vals):\n",
        "        subset = df[df['n_estimators'] == n_est]\n",
        "        curr_color = colors[i % len(colors)]\n",
        "\n",
        "        plt.plot(subset['max_depth'], subset[val_col],\n",
        "                 marker='o', label=f'Val (trees={n_est})',\n",
        "                 color=curr_color, linestyle='-')\n",
        "\n",
        "        plt.plot(subset['max_depth'], subset[train_col],\n",
        "                 label=f'Train (trees={n_est})',\n",
        "                 color=curr_color, linestyle='--', alpha=0.6)\n",
        "\n",
        "    plt.title(title, fontsize=14)\n",
        "    plt.xlabel('Max Depth', fontsize=12)\n",
        "    plt.ylabel(y_label, fontsize=12)\n",
        "    plt.xticks(df['max_depth'].unique())\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.legend(loc='best')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "jMNq8814_5AZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Tree"
      ],
      "metadata": {
        "id": "mON5qGNi_xVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "depth_range = [5, 10, 15, 20, 25]\n",
        "split_range = [5, 10, 15, 20, 25]\n",
        "\n",
        "results_list = []\n",
        "\n",
        "for min_split in split_range:\n",
        "    for depth in depth_range:\n",
        "\n",
        "        print(f\"Training DT: Depth={depth}, Split={min_split}..\")\n",
        "\n",
        "        dt = DecisionTreeClassifier(\n",
        "            max_depth=depth,\n",
        "            min_samples_split=min_split,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        dt.fit(X_train, y_train)\n",
        "\n",
        "        y_train_probs = dt.predict_proba(X_train)\n",
        "        train_metrics = evaluate_model(y_train, y_train_probs, is_multilabel=False, model_name=\"DT\", split_name=\"Train\")\n",
        "\n",
        "        y_val_probs = dt.predict_proba(X_val)\n",
        "        val_metrics = evaluate_model(y_val, y_val_probs, is_multilabel=False, model_name=\"DT\", split_name=\"Val\")\n",
        "\n",
        "        results_list.append({\n",
        "            'max_depth': depth,\n",
        "            'min_samples_split': min_split,\n",
        "\n",
        "            'Train_AUC': train_metrics['AUC-ROC'],\n",
        "            'Val_AUC': val_metrics['AUC-ROC'],\n",
        "\n",
        "            'Train_Top5': train_metrics['Top-5'],\n",
        "            'Val_Top5': val_metrics['Top-5'],\n",
        "\n",
        "            'Train_Top10': train_metrics['Top-10'],\n",
        "            'Val_Top10': val_metrics['Top-10']\n",
        "        })\n",
        "\n",
        "df_plot = pd.DataFrame(results_list)"
      ],
      "metadata": {
        "id": "Xqw0M-yq_y5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_metric(df_plot, 'Train_AUC', 'Val_AUC',\n",
        "            'DT Hyperparameter Search: AUC-ROC', 'AUC-ROC Score')\n",
        "\n",
        "plot_metric(df_plot, 'Train_Top5', 'Val_Top5',\n",
        "            'DT Hyperparameter Search: Top-5 Accuracy', 'Top-5 Score')\n",
        "\n",
        "plot_metric(df_plot, 'Train_Top10', 'Val_Top10',\n",
        "            'DT Hyperparameter Search: Top-10 Accuracy', 'Top-10 Score')"
      ],
      "metadata": {
        "id": "4Z4Cokwr_7eD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest Classifier"
      ],
      "metadata": {
        "id": "1ZfT8IonAIAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "estimator_range = [50, 100, 200]\n",
        "depth_range = [5, 8, 10]\n",
        "split_range = [5, 10]\n",
        "\n",
        "rf_results_list = []\n",
        "\n",
        "for min_split in split_range:\n",
        "    for depth in depth_range:\n",
        "      for estimate in estimator_range:\n",
        "          print(f\"Training RF: Depth={depth}, Split={min_split}, Estimator={estimate}...\")\n",
        "\n",
        "          rf = RandomForestClassifier(\n",
        "              n_estimators=estimate,\n",
        "              max_depth=depth,\n",
        "              min_samples_split=min_split,\n",
        "              class_weight=class_weights,\n",
        "              random_state=42,\n",
        "              n_jobs=-1\n",
        "          )\n",
        "\n",
        "          rf.fit(X_train, y_train)\n",
        "\n",
        "          y_train_probs = rf.predict_proba(X_train)\n",
        "          train_metrics = evaluate_model(y_train, y_train_probs, is_multilabel=False, model_name=\"RF\", split_name=\"Train\")\n",
        "\n",
        "          y_val_probs = rf.predict_proba(X_val)\n",
        "          val_metrics = evaluate_model(y_val, y_val_probs, is_multilabel=False, model_name=\"RF\", split_name=\"Val\")\n",
        "\n",
        "          rf_results_list.append({\n",
        "              'max_depth': depth,\n",
        "              'min_samples_split': min_split,\n",
        "\n",
        "              'Train_AUC': train_metrics['AUC-ROC'],\n",
        "              'Val_AUC': val_metrics['AUC-ROC'],\n",
        "\n",
        "              'Train_Top5': train_metrics['Top-5'],\n",
        "              'Val_Top5': val_metrics['Top-5'],\n",
        "\n",
        "              'Train_Top10': train_metrics['Top-10'],\n",
        "              'Val_Top10': val_metrics['Top-10']\n",
        "          })\n",
        "\n",
        "df_rf_plot = pd.DataFrame(rf_results_list)"
      ],
      "metadata": {
        "id": "6ZLF5wo1_zr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_metric(df_rf_plot, 'Train_AUC', 'Val_AUC',\n",
        "            'RF Hyperparameter Search: AUC-ROC', 'AUC-ROC Score')\n",
        "\n",
        "plot_metric(df_rf_plot, 'Train_Top5', 'Val_Top5',\n",
        "            'RF Hyperparameter Search: Top-5 Accuracy', 'Top-5 Score')\n",
        "\n",
        "plot_metric(df_rf_plot, 'Train_Top10', 'Val_Top10',\n",
        "            'RF Hyperparameter Search: Top-10 Accuracy', 'Top-10 Score')"
      ],
      "metadata": {
        "id": "CihFIH3U_929"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost Classifier"
      ],
      "metadata": {
        "id": "q7iz0HwiAR2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "estimators_range = [50, 100]\n",
        "depth_range = [5, 6]\n",
        "\n",
        "xgb_results_list = []\n",
        "\n",
        "\n",
        "for n_est in estimators_range:\n",
        "    for depth in depth_range:\n",
        "        print(f\"Training XGB: Trees={n_est}, Depth={depth}...\")\n",
        "\n",
        "        xgb = XGBClassifier(\n",
        "            n_estimators=n_est,\n",
        "            max_depth=depth,\n",
        "            learning_rate=0.1,\n",
        "            random_state=42,\n",
        "            n_jobs=-1,\n",
        "            eval_metric='mlogloss'\n",
        "        )\n",
        "        xgb, xgb_time, xgb_mem = train_and_evaluate(xgb, \"XGBoost\", X_train, y_train)\n",
        "\n",
        "        y_train_probs = xgb.predict_proba(X_train)\n",
        "        train_metrics = evaluate_model(y_train, y_train_probs, is_multilabel=False, model_name=\"XGB\", split_name=\"Train\")\n",
        "\n",
        "        y_val_probs = xgb.predict_proba(X_val)\n",
        "        val_metrics = evaluate_model(y_val, y_val_probs, is_multilabel=False, model_name=\"XGB\", split_name=\"Val\")\n",
        "\n",
        "        xgb_results_list.append({\n",
        "            'n_estimators': n_est,\n",
        "            'max_depth': depth,\n",
        "\n",
        "            'Train_AUC': train_metrics['AUC-ROC'],\n",
        "            'Val_AUC': val_metrics['AUC-ROC'],\n",
        "\n",
        "            'Train_Top5': train_metrics['Top-5'],\n",
        "            'Val_Top5': val_metrics['Top-5'],\n",
        "\n",
        "            'Train_Top10': train_metrics['Top-10'],\n",
        "            'Val_Top10': val_metrics['Top-10']\n",
        "        })\n",
        "\n",
        "df_xgb_plot = pd.DataFrame(xgb_results_list)"
      ],
      "metadata": {
        "id": "WYk5bVsI_2VB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_xgb_metric(df_xgb_plot, 'Train_AUC', 'Val_AUC',\n",
        "            'XGBoost Hyperparameter Search: AUC-ROC', 'AUC-ROC Score')\n",
        "\n",
        "plot_xgb_metric(df_xgb_plot, 'Train_Top5', 'Val_Top5',\n",
        "            'XGBoost Hyperparameter Search: Top-5 Accuracy', 'Top-5 Score')\n",
        "\n",
        "plot_xgb_metric(df_xgb_plot, 'Train_Top10', 'Val_Top10',\n",
        "            'XGBoost Hyperparameter Search: Top-10 Accuracy', 'Top-10 Score')"
      ],
      "metadata": {
        "id": "SXJVSZR3AAep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extra Tree Classifier"
      ],
      "metadata": {
        "id": "GXivApS6AT7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "max_depth_list = [5, 10, 20, 30, 50]\n",
        "min_split_list = [5, 10, 20]\n",
        "\n",
        "hp_results = []\n",
        "\n",
        "\n",
        "for min_split in min_split_list:\n",
        "    for max_depth in max_depth_list:\n",
        "        print(f\"\\nTesting max_depth={max_depth}, min_samples_split={min_split}\")\n",
        "\n",
        "        fold_val_top5 = []\n",
        "        fold_val_top10 = []\n",
        "        fold_train_top5 = []\n",
        "        fold_train_top10 = []\n",
        "        fold_auc = []\n",
        "\n",
        "        for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train), 1):\n",
        "\n",
        "            X_train_split, X_val_split = X_train[train_idx], X_train[val_idx]\n",
        "            y_train_split, y_val_split = y_train[train_idx], y_train[val_idx]\n",
        "\n",
        "            class_counts = Counter(y_train_split)\n",
        "            class_weights = {label: 1 / np.sqrt(count) for label, count in class_counts.items()}\n",
        "            sample_weights = np.array([class_weights[label] for label in y_train_split])\n",
        "\n",
        "            model = ExtraTreesClassifier(\n",
        "                n_estimators=100,\n",
        "                max_depth=max_depth,\n",
        "                min_samples_split=min_split,\n",
        "                min_samples_leaf=5,\n",
        "                class_weight=class_weights,\n",
        "                random_state=42,\n",
        "                n_jobs=-1\n",
        "            )\n",
        "\n",
        "            model.fit(X_train_split, y_train_split, sample_weight=sample_weights)\n",
        "\n",
        "            y_val_pred = model.predict_proba(X_val_split)\n",
        "            y_train_pred = model.predict_proba(X_train_split)\n",
        "\n",
        "            val_top5 = top_k_accuracy_score(y_val_split, y_val_pred, k=5)\n",
        "            val_top10 = top_k_accuracy_score(y_val_split, y_val_pred, k=10)\n",
        "\n",
        "            train_top5 = top_k_accuracy_score(y_train_split, y_train_pred, k=5)\n",
        "            train_top10 = top_k_accuracy_score(y_train_split, y_train_pred, k=10)\n",
        "\n",
        "            auc_roc = roc_auc_score(\n",
        "                label_binarize(y_val_split, classes=np.unique(y_train)),\n",
        "                y_val_pred,\n",
        "                average='weighted',\n",
        "                multi_class='ovr'\n",
        "            )\n",
        "\n",
        "            fold_val_top5.append(val_top5)\n",
        "            fold_val_top10.append(val_top10)\n",
        "            fold_train_top5.append(train_top5)\n",
        "            fold_train_top10.append(train_top10)\n",
        "            fold_auc.append(auc_roc)\n",
        "\n",
        "        hp_results.append({\n",
        "            \"max_depth\": max_depth,\n",
        "            \"min_split\": min_split,\n",
        "            \"train_top5_mean\": np.mean(fold_train_top5),\n",
        "            \"train_top10_mean\": np.mean(fold_train_top10),\n",
        "            \"val_top5_mean\": np.mean(fold_val_top5),\n",
        "            \"val_top10_mean\": np.mean(fold_val_top10),\n",
        "            \"auc_mean\": np.mean(fold_auc),\n",
        "        })\n",
        "\n",
        "hp_df = pd.DataFrame(hp_results)\n",
        "print(\"Hyperparameter Search Completed!\")\n",
        "hp_df"
      ],
      "metadata": {
        "id": "w2EHThqJAdoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,7))\n",
        "\n",
        "jitter = {5: -1.0, 10: 0, 20: 1.0}\n",
        "\n",
        "for min_split in min_split_list:\n",
        "    subdf = hp_df[hp_df[\"min_split\"] == min_split]\n",
        "\n",
        "    plt.plot(\n",
        "        subdf[\"max_depth\"] + jitter[min_split],\n",
        "        subdf[\"val_top10_mean\"],\n",
        "        marker=\"o\",\n",
        "        linestyle=\"--\",\n",
        "        label=f\"Val Top-10 (min_split={min_split})\"\n",
        "    )\n",
        "\n",
        "    plt.plot(\n",
        "        subdf[\"max_depth\"] + jitter[min_split],\n",
        "        subdf[\"train_top10_mean\"],\n",
        "        marker=\"s\",\n",
        "        linestyle=\"-\",\n",
        "        label=f\"Train Top-10 (min_split={min_split})\"\n",
        "    )\n",
        "\n",
        "plt.title(\"Extra Trees Hyperparameter Search (Top-10 Accuracy: Train vs Validation)\")\n",
        "plt.xlabel(\"Max Depth\")\n",
        "plt.ylabel(\"Top-10 Accuracy\")\n",
        "plt.legend(ncol=2)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ucljK7tJAfuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,7))\n",
        "\n",
        "for min_split in min_split_list:\n",
        "    subdf = hp_df[hp_df[\"min_split\"] == min_split]\n",
        "\n",
        "    # Validation Top-5\n",
        "    plt.plot(\n",
        "        subdf[\"max_depth\"] + jitter[min_split],\n",
        "        subdf[\"val_top5_mean\"],\n",
        "        marker=\"o\",\n",
        "        linestyle=\"--\",\n",
        "        label=f\"Val Top-5 (min_split={min_split})\"\n",
        "    )\n",
        "\n",
        "    # Training Top-5\n",
        "    plt.plot(\n",
        "        subdf[\"max_depth\"] + jitter[min_split],\n",
        "        subdf[\"train_top5_mean\"],\n",
        "        marker=\"s\",\n",
        "        linestyle=\"-\",\n",
        "        label=f\"Train Top-5 (min_split={min_split})\"\n",
        "    )\n",
        "\n",
        "plt.title(\"Extra Trees Hyperparameter Search (Top-5 Accuracy: Train vs Validation)\")\n",
        "plt.xlabel(\"Max Depth\")\n",
        "plt.ylabel(\"Top-5 Accuracy\")\n",
        "plt.legend(ncol=2)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sMTN7WoGAiEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.metrics import roc_auc_score, top_k_accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import psutil, os, time\n",
        "\n",
        "\n",
        "def top_k_acc(model, X, y, k):\n",
        "    \"\"\"Compute top-k accuracy.\"\"\"\n",
        "    probs = model.predict_proba(X)\n",
        "    return top_k_accuracy_score(y, probs, k=k, labels=np.arange(probs.shape[1]))\n",
        "\n",
        "def train_and_evaluate(model, X_train, y_train, X_val, y_val):\n",
        "    \"\"\"Returns metrics for train & val.\"\"\"\n",
        "\n",
        "    process = psutil.Process(os.getpid())\n",
        "    mem_before = process.memory_info().rss / 1024 / 1024\n",
        "\n",
        "    start = time.time()\n",
        "    model.fit(X_train, y_train)\n",
        "    train_time = time.time() - start\n",
        "\n",
        "    mem_after = process.memory_info().rss / 1024 / 1024\n",
        "    mem_used = mem_after - mem_before\n",
        "\n",
        "    train_probs = model.predict_proba(X_train)\n",
        "    val_probs   = model.predict_proba(X_val)\n",
        "\n",
        "    results = {\n",
        "        \"train_top5\": top_k_accuracy_score(y_train, train_probs, k=5),\n",
        "        \"val_top5\":   top_k_accuracy_score(y_val,   val_probs,   k=5),\n",
        "\n",
        "        \"train_top10\": top_k_accuracy_score(y_train, train_probs, k=10),\n",
        "        \"val_top10\":   top_k_accuracy_score(y_val,   val_probs,   k=10),\n",
        "\n",
        "        \"train_auc\": roc_auc_score(y_train, train_probs, multi_class='ovr'),\n",
        "        \"val_auc\":   roc_auc_score(y_val,   val_probs,   multi_class='ovr'),\n",
        "\n",
        "        \"train_time\": train_time,\n",
        "        \"mem_used_MB\": mem_used\n",
        "    }\n",
        "    return results\n",
        "\n",
        "max_depth_list = [5, 8, 10]\n",
        "split_list     = [2, 5, 10]\n",
        "\n",
        "results = []\n",
        "\n",
        "for depth in max_depth_list:\n",
        "    for split in split_list:\n",
        "        print(f\"Running ExtraTrees with depth={depth}, min_samples_split={split}\")\n",
        "\n",
        "        model = ExtraTreesClassifier(\n",
        "            n_estimators=100,\n",
        "            max_depth=depth,\n",
        "            min_samples_split=split,\n",
        "            class_weight=class_weights,\n",
        "            n_jobs=-1,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        metrics = train_and_evaluate(model, X_train, y_train, X_val, y_val)\n",
        "        metrics[\"max_depth\"] = depth\n",
        "        metrics[\"min_samples_split\"] = split\n",
        "\n",
        "        results.append(metrics)\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "print(df)"
      ],
      "metadata": {
        "id": "hQxKJo60AkAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_metric(df, metric_train, metric_val, title):\n",
        "    plt.figure(figsize=(10,6))\n",
        "\n",
        "    for split in split_list:\n",
        "        subset = df[df[\"min_samples_split\"] == split]\n",
        "\n",
        "        plt.plot(subset[\"max_depth\"], subset[metric_train],\n",
        "                 marker='o', label=f\"Train - split={split}\")\n",
        "\n",
        "        plt.plot(subset[\"max_depth\"], subset[metric_val],\n",
        "                 marker='o', linestyle='--', label=f\"Val - split={split}\")\n",
        "\n",
        "    plt.xlabel(\"max_depth\")\n",
        "    plt.ylabel(metric_train.replace(\"_\", \" \").title())\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "plot_metric(df, \"train_top5\", \"val_top5\",\n",
        "            \"ExtraTrees Hyperparameter Search — Top-5 Accuracy\")\n",
        "\n",
        "plot_metric(df, \"train_top10\", \"val_top10\",\n",
        "            \"ExtraTrees Hyperparameter Search — Top-10 Accuracy\")\n",
        "\n",
        "plot_metric(df, \"train_auc\", \"val_auc\",\n",
        "            \"ExtraTrees Hyperparameter Search — AUC-ROC\")"
      ],
      "metadata": {
        "id": "q-zf_eRzAptl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stratified Cross Validation with best parameters for each model"
      ],
      "metadata": {
        "id": "fM0mIR2skY0w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Tree"
      ],
      "metadata": {
        "id": "3RazLvzAnjC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import top_k_accuracy_score, roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import time\n",
        "\n",
        "# Initialize\n",
        "n_splits = 5\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "# Storage for results\n",
        "cv_results = {\n",
        "    'fold': [],\n",
        "    'top5_acc': [],\n",
        "    'top10_acc': [],\n",
        "    'auc_roc': [],\n",
        "    'species_coverage': [],\n",
        "    'training_time': []\n",
        "}\n",
        "\n",
        "# Cross-validation loop\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(X_processed, y_encoded), 1):\n",
        "    print(f\"Training Fold {fold}/{n_splits}...\")\n",
        "\n",
        "    # Split data\n",
        "    X_train_split, X_val_split = X_processed[train_idx], X_processed[val_idx]\n",
        "    y_train_split, y_val_split = y_encoded[train_idx], y_encoded[val_idx]\n",
        "\n",
        "    # Calculate class weights\n",
        "    class_counts = Counter(y_train_split)\n",
        "    class_weights = {label: 1/np.sqrt(count) for label, count in class_counts.items()}\n",
        "    sample_weights = np.array([class_weights[label] for label in y_train_split])\n",
        "\n",
        "    # Train Decision Tree\n",
        "    start_time = time.time()\n",
        "\n",
        "    dt = DecisionTreeClassifier(\n",
        "        max_depth=15,\n",
        "        min_samples_split=10,\n",
        "        min_samples_leaf=5,\n",
        "        class_weight=class_weights,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    dt.fit(X_train_split, y_train_split, sample_weight=sample_weights)\n",
        "    training_time = time.time() - start_time\n",
        "\n",
        "    # Predictions\n",
        "    y_pred_proba = dt.predict_proba(X_val_split)\n",
        "\n",
        "    # Calculate metrics\n",
        "    top5 = top_k_accuracy_score(y_val_split, y_pred_proba, k=5)\n",
        "    top10 = top_k_accuracy_score(y_val_split, y_pred_proba, k=10)\n",
        "\n",
        "    # AUC-ROC\n",
        "    y_val_binarized = label_binarize(y_val_split, classes=np.unique(y_train))\n",
        "    if y_val_binarized.shape[1] == y_pred_proba.shape[1]:\n",
        "        auc_roc = roc_auc_score(y_val_binarized, y_pred_proba, average='weighted', multi_class='ovr')\n",
        "    else:\n",
        "        common_classes = np.unique(y_val_split)\n",
        "        y_val_binarized_subset = label_binarize(y_val_split, classes=common_classes)\n",
        "        y_pred_proba_subset = y_pred_proba[:, common_classes]\n",
        "        auc_roc = roc_auc_score(y_val_binarized_subset, y_pred_proba_subset, average='weighted', multi_class='ovr')\n",
        "\n",
        "    # Species Coverage\n",
        "    species_coverage = np.mean(np.sum(y_pred_proba > 0.00, axis=1))\n",
        "\n",
        "    # Store results\n",
        "    cv_results['fold'].append(fold)\n",
        "    cv_results['top5_acc'].append(top5)\n",
        "    cv_results['top10_acc'].append(top10)\n",
        "    cv_results['auc_roc'].append(auc_roc)\n",
        "    cv_results['species_coverage'].append(species_coverage)\n",
        "    cv_results['training_time'].append(training_time)\n",
        "\n",
        "    print(f\"  Top-5: {top5:.4f}, Top-10: {top10:.4f}, AUC-ROC: {auc_roc:.4f}, Coverage: {species_coverage:.2f}\")\n",
        "\n",
        "cv_df_dt = pd.DataFrame(cv_results)\n",
        "\n",
        "# Summary statistics\n",
        "summary_dt = {\n",
        "    'Model': 'Decision Tree',\n",
        "    'Top-5 Mean': cv_df_dt['top5_acc'].mean(),\n",
        "    'Top-5 Std': cv_df_dt['top5_acc'].std(),\n",
        "    'Top-10 Mean': cv_df_dt['top10_acc'].mean(),\n",
        "    'Top-10 Std': cv_df_dt['top10_acc'].std(),\n",
        "    'AUC-ROC Mean': cv_df_dt['auc_roc'].mean(),\n",
        "    'AUC-ROC Std': cv_df_dt['auc_roc'].std(),\n",
        "    'Species Coverage Mean': cv_df_dt['species_coverage'].mean(),\n",
        "    'Species Coverage Std': cv_df_dt['species_coverage'].std(),\n",
        "    'Training Time Mean': cv_df_dt['training_time'].mean(),\n",
        "    'Training Time Std': cv_df_dt['training_time'].std()\n",
        "}\n",
        "\n",
        "print()\n",
        "print(\"Decision Tree Metrics:\")\n",
        "for key, value in summary_dt.items():\n",
        "    if 'Mean' in key or 'Std' in key:\n",
        "        print(f\"{key:25s}: {value:.4f}\")\n",
        "    else:\n",
        "        print(f\"{key:25s}: {value}\")"
      ],
      "metadata": {
        "id": "YeQNPszV2Ptg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest"
      ],
      "metadata": {
        "id": "bC9lpQhutFQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import top_k_accuracy_score, roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Initialize\n",
        "n_splits = 5\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "# Storage for results\n",
        "cv_results = {\n",
        "    'fold': [],\n",
        "    'top5_acc': [],\n",
        "    'top10_acc': [],\n",
        "    'auc_roc': [],\n",
        "    'avg_species_coverage': [],\n",
        "    'training_time': []\n",
        "}\n",
        "\n",
        "# Cross-validation loop\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(X_processed, y_encoded), 1):\n",
        "    print(f\"Training Fold {fold}/{n_splits}\")\n",
        "\n",
        "    # Split data\n",
        "    X_train_split, X_val_split = X_processed[train_idx], X_processed[val_idx]\n",
        "    y_train_split, y_val_split = y_encoded[train_idx], y_encoded[val_idx]\n",
        "\n",
        "    # Calculate class weights\n",
        "    from collections import Counter\n",
        "    class_counts = Counter(y_train_split)\n",
        "    class_weights = {label: 1/np.sqrt(count) for label, count in class_counts.items()}\n",
        "    sample_weights = np.array([class_weights[label] for label in y_train_split])\n",
        "\n",
        "    # Train model\n",
        "    import time\n",
        "    start_time = time.time()\n",
        "\n",
        "    rf = RandomForestClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=5,\n",
        "        min_samples_split=10,\n",
        "        class_weight=class_weights,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    rf.fit(X_train_split, y_train_split, sample_weight=sample_weights)\n",
        "    training_time = time.time() - start_time\n",
        "\n",
        "    # Predictions\n",
        "    y_pred_proba = rf.predict_proba(X_val_split)\n",
        "\n",
        "    # Metrics\n",
        "    top5 = top_k_accuracy_score(y_val_split, y_pred_proba, k=5)\n",
        "    top10 = top_k_accuracy_score(y_val_split, y_pred_proba, k=10)\n",
        "\n",
        "    # AUC-ROC\n",
        "    y_val_binarized = label_binarize(y_val_split, classes=np.unique(y_train))\n",
        "    if y_val_binarized.shape[1] == y_pred_proba.shape[1]:\n",
        "        auc_roc = roc_auc_score(y_val_binarized, y_pred_proba, average='weighted', multi_class='ovr')\n",
        "    else:\n",
        "        common_classes = np.unique(y_val_split)\n",
        "        y_val_binarized_subset = label_binarize(y_val_split, classes=common_classes)\n",
        "        y_pred_proba_subset = y_pred_proba[:, common_classes]\n",
        "        auc_roc = roc_auc_score(y_val_binarized_subset, y_pred_proba_subset, average='weighted', multi_class='ovr')\n",
        "\n",
        "    species_coverage = np.sum(y_pred_proba > 0.00, axis=1)  # Count species with >0% probability per sample\n",
        "    avg_species_coverage = np.mean(species_coverage)\n",
        "\n",
        "    # Store results\n",
        "    cv_results['fold'].append(fold)\n",
        "    cv_results['top5_acc'].append(top5)\n",
        "    cv_results['top10_acc'].append(top10)\n",
        "    cv_results['auc_roc'].append(auc_roc)\n",
        "    cv_results['avg_species_coverage'].append(avg_species_coverage)\n",
        "    cv_results['training_time'].append(training_time)\n",
        "\n",
        "    print(f\"Top-5: {top5:.4f}, Top-10: {top10:.4f}, AUC-ROC: {auc_roc:.4f}, Avg Species Coverage: {avg_species_coverage:.2f}\")\n",
        "\n",
        "# Convert to DataFrame\n",
        "cv_df = pd.DataFrame(cv_results)\n",
        "\n",
        "print(cv_df)\n",
        "\n",
        "for metric in ['top5_acc', 'top10_acc', 'auc_roc', 'avg_species_coverage']:\n",
        "    print(f\"{metric:20s}: {cv_df[metric].mean():.4f} ± {cv_df[metric].std():.4f}\")\n"
      ],
      "metadata": {
        "id": "BjPiQTrRniWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost Classifier"
      ],
      "metadata": {
        "id": "tll2dOcc7Oam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import top_k_accuracy_score, roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from xgboost import XGBClassifier\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import time\n",
        "\n",
        "# Initialize\n",
        "n_splits = 5\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "# Storage for results\n",
        "cv_results = {\n",
        "    'fold': [],\n",
        "    'top5_acc': [],\n",
        "    'top10_acc': [],\n",
        "    'auc_roc': [],\n",
        "    'species_coverage': [],\n",
        "    'training_time': []\n",
        "}\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(X_processed, y_encoded), 1):\n",
        "    print(f\"Training Fold {fold}/{n_splits}...\")\n",
        "\n",
        "    # Split data\n",
        "    X_train_split, X_val_split = X_processed[train_idx], X_processed[val_idx]\n",
        "    y_train_split, y_val_split = y_encoded[train_idx], y_encoded[val_idx]\n",
        "\n",
        "    # Calculate class weights\n",
        "    class_counts = Counter(y_train_split)\n",
        "    class_weights = {label: 1/np.sqrt(count) for label, count in class_counts.items()}\n",
        "    sample_weights = np.array([class_weights[label] for label in y_train_split])\n",
        "\n",
        "    # Train XGBoost\n",
        "    start_time = time.time()\n",
        "\n",
        "    xgb = XGBClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,\n",
        "        class_weights=class_weights,\n",
        "        objective='multi:softprob',  # For soft probabilities, model calibration\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "        eval_metric='mlogloss'\n",
        "        )\n",
        "\n",
        "    xgb.fit(X_train_split, y_train_split, sample_weight=sample_weights)\n",
        "    training_time = time.time() - start_time\n",
        "\n",
        "    # Predictions\n",
        "    y_pred_proba = xgb.predict_proba(X_val_split)\n",
        "\n",
        "    # Calculate metrics\n",
        "    top5 = top_k_accuracy_score(y_val_split, y_pred_proba, k=5)\n",
        "    top10 = top_k_accuracy_score(y_val_split, y_pred_proba, k=10)\n",
        "\n",
        "    # AUC-ROC\n",
        "    y_val_binarized = label_binarize(y_val_split, classes=np.unique(y_train))\n",
        "    if y_val_binarized.shape[1] == y_pred_proba.shape[1]:\n",
        "        auc_roc = roc_auc_score(y_val_binarized, y_pred_proba, average='weighted', multi_class='ovr')\n",
        "    else:\n",
        "        common_classes = np.unique(y_val_split)\n",
        "        y_val_binarized_subset = label_binarize(y_val_split, classes=common_classes)\n",
        "        y_pred_proba_subset = y_pred_proba[:, common_classes]\n",
        "        auc_roc = roc_auc_score(y_val_binarized_subset, y_pred_proba_subset, average='weighted', multi_class='ovr')\n",
        "\n",
        "    # Species Coverage\n",
        "    species_coverage = np.mean(np.sum(y_pred_proba > 0.00, axis=1))\n",
        "\n",
        "    # Store results\n",
        "    cv_results['fold'].append(fold)\n",
        "    cv_results['top5_acc'].append(top5)\n",
        "    cv_results['top10_acc'].append(top10)\n",
        "    cv_results['auc_roc'].append(auc_roc)\n",
        "    cv_results['species_coverage'].append(species_coverage)\n",
        "    cv_results['training_time'].append(training_time)\n",
        "\n",
        "    print(f\"  Top-5: {top5:.4f}, Top-10: {top10:.4f}, AUC-ROC: {auc_roc:.4f}, Coverage: {species_coverage:.2f}\")\n",
        "\n",
        "# Convert to DataFrame\n",
        "cv_df_xgb = pd.DataFrame(cv_results)\n",
        "\n",
        "# Summary statistics\n",
        "summary_xgb = {\n",
        "    'Model': 'XGBoost',\n",
        "    'Top-5 Mean': cv_df_xgb['top5_acc'].mean(),\n",
        "    'Top-5 Std': cv_df_xgb['top5_acc'].std(),\n",
        "    'Top-10 Mean': cv_df_xgb['top10_acc'].mean(),\n",
        "    'Top-10 Std': cv_df_xgb['top10_acc'].std(),\n",
        "    'AUC-ROC Mean': cv_df_xgb['auc_roc'].mean(),\n",
        "    'AUC-ROC Std': cv_df_xgb['auc_roc'].std(),\n",
        "    'Species Coverage Mean': cv_df_xgb['species_coverage'].mean(),\n",
        "    'Species Coverage Std': cv_df_xgb['species_coverage'].std(),\n",
        "    'Training Time Mean': cv_df_xgb['training_time'].mean(),\n",
        "    'Training Time Std': cv_df_xgb['training_time'].std()\n",
        "}\n",
        "\n",
        "print(\"XGBoost - Summary:\")\n",
        "for key, value in summary_xgb.items():\n",
        "    if 'Mean' in key or 'Std' in key:\n",
        "        print(f\"{key:25s}: {value:.4f}\")\n",
        "    else:\n",
        "        print(f\"{key:25s}: {value}\")"
      ],
      "metadata": {
        "id": "H9WLel4TrbYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extra Tree Classifier"
      ],
      "metadata": {
        "id": "wbxGxNx47w0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import top_k_accuracy_score, roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import time\n",
        "\n",
        "# Initialize\n",
        "n_splits = 5\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "# Storage for results\n",
        "cv_results = {\n",
        "    'fold': [],\n",
        "    'top5_acc': [],\n",
        "    'top10_acc': [],\n",
        "    'auc_roc': [],\n",
        "    'species_coverage': [],\n",
        "    'training_time': []\n",
        "}\n",
        "\n",
        "# Cross-validation loop\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(X_processed, y_encoded), 1):\n",
        "    print(f\"Training Fold {fold}/{n_splits}...\")\n",
        "\n",
        "    # Split data\n",
        "    X_train_split, X_val_split = X_processed[train_idx], X_processed[val_idx]\n",
        "    y_train_split, y_val_split = y_encoded[train_idx], y_encoded[val_idx]\n",
        "\n",
        "    # Calculate class weights\n",
        "    class_counts = Counter(y_train_split)\n",
        "    class_weights = {label: 1/np.sqrt(count) for label, count in class_counts.items()}\n",
        "    sample_weights = np.array([class_weights[label] for label in y_train_split])\n",
        "\n",
        "    # Train Extra Trees\n",
        "    start_time = time.time()\n",
        "\n",
        "    et = ExtraTreesClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=5,\n",
        "        min_samples_split=10,\n",
        "        min_samples_leaf=5,\n",
        "        class_weight=class_weights,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    et.fit(X_train_split, y_train_split, sample_weight=sample_weights)\n",
        "    training_time = time.time() - start_time\n",
        "\n",
        "    # Predictions\n",
        "    y_pred_proba = et.predict_proba(X_val_split)\n",
        "\n",
        "    # Calculate metrics\n",
        "    top5 = top_k_accuracy_score(y_val_split, y_pred_proba, k=5)\n",
        "    top10 = top_k_accuracy_score(y_val_split, y_pred_proba, k=10)\n",
        "\n",
        "    # AUC-ROC\n",
        "    y_val_binarized = label_binarize(y_val_split, classes=np.unique(y_train))\n",
        "    if y_val_binarized.shape[1] == y_pred_proba.shape[1]:\n",
        "        auc_roc = roc_auc_score(y_val_binarized, y_pred_proba, average='weighted', multi_class='ovr')\n",
        "    else:\n",
        "        common_classes = np.unique(y_val_split)\n",
        "        y_val_binarized_subset = label_binarize(y_val_split, classes=common_classes)\n",
        "        y_pred_proba_subset = y_pred_proba[:, common_classes]\n",
        "        auc_roc = roc_auc_score(y_val_binarized_subset, y_pred_proba_subset, average='weighted', multi_class='ovr')\n",
        "\n",
        "    # Species Coverage\n",
        "    species_coverage = np.mean(np.sum(y_pred_proba > 0.01, axis=1))\n",
        "\n",
        "    # Store results\n",
        "    cv_results['fold'].append(fold)\n",
        "    cv_results['top5_acc'].append(top5)\n",
        "    cv_results['top10_acc'].append(top10)\n",
        "    cv_results['auc_roc'].append(auc_roc)\n",
        "    cv_results['species_coverage'].append(species_coverage)\n",
        "    cv_results['training_time'].append(training_time)\n",
        "\n",
        "    print(f\"  Top-5: {top5:.4f}, Top-10: {top10:.4f}, AUC-ROC: {auc_roc:.4f}, Coverage: {species_coverage:.2f}\")\n",
        "\n",
        "# Convert to DataFrame\n",
        "cv_df_et = pd.DataFrame(cv_results)\n",
        "\n",
        "# Summary statistics\n",
        "summary_et = {\n",
        "    'Model': 'Extra Trees',\n",
        "    'Top-5 Mean': cv_df_et['top5_acc'].mean(),\n",
        "    'Top-5 Std': cv_df_et['top5_acc'].std(),\n",
        "    'Top-10 Mean': cv_df_et['top10_acc'].mean(),\n",
        "    'Top-10 Std': cv_df_et['top10_acc'].std(),\n",
        "    'AUC-ROC Mean': cv_df_et['auc_roc'].mean(),\n",
        "    'AUC-ROC Std': cv_df_et['auc_roc'].std(),\n",
        "    'Species Coverage Mean': cv_df_et['species_coverage'].mean(),\n",
        "    'Species Coverage Std': cv_df_et['species_coverage'].std(),\n",
        "    'Training Time Mean': cv_df_et['training_time'].mean(),\n",
        "    'Training Time Std': cv_df_et['training_time'].std()\n",
        "}\n",
        "\n",
        "print()\n",
        "print(\"Extra Trees - Summary:\")\n",
        "for key, value in summary_et.items():\n",
        "    if 'Mean' in key or 'Std' in key:\n",
        "        print(f\"{key:25s}: {value:.4f}\")\n",
        "    else:\n",
        "        print(f\"{key:25s}: {value}\")"
      ],
      "metadata": {
        "id": "yvaiHalKG5BH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Light Gradient Boost Model"
      ],
      "metadata": {
        "id": "WxayK6vLC9WC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import top_k_accuracy_score, roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from lightgbm import LGBMClassifier\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import time\n",
        "\n",
        "# Initialize\n",
        "n_splits = 5\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "# Storage for results\n",
        "cv_results = {\n",
        "    'fold': [],\n",
        "    'top5_acc': [],\n",
        "    'top10_acc': [],\n",
        "    'auc_roc': [],\n",
        "    'species_coverage': [],\n",
        "    'training_time': []\n",
        "}\n",
        "\n",
        "# Cross-validation loop\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(X_processed, y_encoded), 1):\n",
        "    print(f\"Training Fold {fold}/{n_splits}...\")\n",
        "\n",
        "    # Split data\n",
        "    X_train_split, X_val_split = X_processed[train_idx], X_processed[val_idx]\n",
        "    y_train_split, y_val_split = y_encoded[train_idx], y_encoded[val_idx]\n",
        "\n",
        "    # Calculate class weights\n",
        "    class_counts = Counter(y_train_split)\n",
        "    class_weights = {label: 1/np.sqrt(count) for label, count in class_counts.items()}\n",
        "    sample_weights = np.array([class_weights[label] for label in y_train_split])\n",
        "\n",
        "    # Train LightGBM\n",
        "    start_time = time.time()\n",
        "\n",
        "    lgbm = LGBMClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=5,\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        class_weight=class_weights,\n",
        "        objective='multiclass',\n",
        "        num_class=500,  # Number of classes\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "        verbosity=-1\n",
        "    )\n",
        "\n",
        "    lgbm.fit(X_train_split, y_train_split, sample_weight=sample_weights)\n",
        "    training_time = time.time() - start_time\n",
        "\n",
        "    # Predictions\n",
        "    y_pred_proba = lgbm.predict_proba(X_val_split)\n",
        "\n",
        "    # Calculate metrics\n",
        "    top5 = top_k_accuracy_score(y_val_split, y_pred_proba, k=5)\n",
        "    top10 = top_k_accuracy_score(y_val_split, y_pred_proba, k=10)\n",
        "\n",
        "    # AUC-ROC\n",
        "    y_val_binarized = label_binarize(y_val_split, classes=np.unique(y_train))\n",
        "    if y_val_binarized.shape[1] == y_pred_proba.shape[1]:\n",
        "        auc_roc = roc_auc_score(y_val_binarized, y_pred_proba, average='weighted', multi_class='ovr')\n",
        "    else:\n",
        "        common_classes = np.unique(y_val_split)\n",
        "        y_val_binarized_subset = label_binarize(y_val_split, classes=common_classes)\n",
        "        y_pred_proba_subset = y_pred_proba[:, common_classes]\n",
        "        auc_roc = roc_auc_score(y_val_binarized_subset, y_pred_proba_subset, average='weighted', multi_class='ovr')\n",
        "\n",
        "    # Species Coverage\n",
        "    species_coverage = np.mean(np.sum(y_pred_proba > 0.01, axis=1))\n",
        "\n",
        "    # Store results\n",
        "    cv_results['fold'].append(fold)\n",
        "    cv_results['top5_acc'].append(top5)\n",
        "    cv_results['top10_acc'].append(top10)\n",
        "    cv_results['auc_roc'].append(auc_roc)\n",
        "    cv_results['species_coverage'].append(species_coverage)\n",
        "    cv_results['training_time'].append(training_time)\n",
        "\n",
        "    print(f\"  Top-5: {top5:.4f}, Top-10: {top10:.4f}, AUC-ROC: {auc_roc:.4f}, Coverage: {species_coverage:.2f}\")\n",
        "\n",
        "# Convert to DataFrame\n",
        "cv_df_lgbm = pd.DataFrame(cv_results)\n",
        "\n",
        "# Summary statistics\n",
        "summary_lgbm = {\n",
        "    'Model': 'LightGBM',\n",
        "    'Top-5 Mean': cv_df_lgbm['top5_acc'].mean(),\n",
        "    'Top-5 Std': cv_df_lgbm['top5_acc'].std(),\n",
        "    'Top-10 Mean': cv_df_lgbm['top10_acc'].mean(),\n",
        "    'Top-10 Std': cv_df_lgbm['top10_acc'].std(),\n",
        "    'AUC-ROC Mean': cv_df_lgbm['auc_roc'].mean(),\n",
        "    'AUC-ROC Std': cv_df_lgbm['auc_roc'].std(),\n",
        "    'Species Coverage Mean': cv_df_lgbm['species_coverage'].mean(),\n",
        "    'Species Coverage Std': cv_df_lgbm['species_coverage'].std(),\n",
        "    'Training Time Mean': cv_df_lgbm['training_time'].mean(),\n",
        "    'Training Time Std': cv_df_lgbm['training_time'].std()\n",
        "}\n",
        "\n",
        "print()\n",
        "print(\"LightGBM - Summary:\")\n",
        "for key, value in summary_lgbm.items():\n",
        "    if 'Mean' in key or 'Std' in key:\n",
        "        print(f\"{key:25s}: {value:.4f}\")\n",
        "    else:\n",
        "        print(f\"{key:25s}: {value}\")"
      ],
      "metadata": {
        "id": "zS-LGUfnsSHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating Test set"
      ],
      "metadata": {
        "id": "ORGZH2ZC869w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_test_set(model, X_test, y_test_multilabel, model_name):\n",
        "    y_pred_proba = model.predict_proba(X_test)\n",
        "\n",
        "    # Top-5 and Top-10\n",
        "    top5_hits = []\n",
        "    top10_hits = []\n",
        "\n",
        "    for i in range(len(y_test_multilabel)):\n",
        "        true_species = set(np.where(y_test_multilabel[i] == 1)[0])\n",
        "        top5_pred = set(np.argsort(y_pred_proba[i])[-5:][::-1])\n",
        "        top10_pred = set(np.argsort(y_pred_proba[i])[-10:][::-1])\n",
        "\n",
        "        top5_hits.append(len(true_species & top5_pred) > 0)\n",
        "        top10_hits.append(len(true_species & top10_pred) > 0)\n",
        "\n",
        "    # AUC-ROC\n",
        "    try:\n",
        "        auc_roc = roc_auc_score(y_test_multilabel, y_pred_proba, average='weighted', multi_class='ovr')\n",
        "    except:\n",
        "        auc_roc = np.nan\n",
        "\n",
        "    # Species Coverage\n",
        "    species_coverage = np.mean(np.sum(y_pred_proba > 0.01, axis=1))\n",
        "\n",
        "    return {\n",
        "        'Model': model_name,\n",
        "        'Top-5': np.mean(top5_hits),\n",
        "        'Top-10': np.mean(top10_hits),\n",
        "        'AUC-ROC': auc_roc,\n",
        "        'Species Coverage': species_coverage\n",
        "    }\n",
        "\n",
        "# Evaluate your models\n",
        "test_results = []\n",
        "\n",
        "test_results.append(evaluate_test_set(dt, X_test_processed, y_test_multilabel, 'Decision Tree'))\n",
        "test_results.append(evaluate_test_set(rf, X_test_processed, y_test_multilabel, 'Random Forest'))\n",
        "test_results.append(evaluate_test_set(xgb, X_test_processed, y_test_multilabel, 'XGBoost'))\n",
        "\n",
        "# Create DataFrame\n",
        "test_df = pd.DataFrame(test_results)\n",
        "print(test_df)"
      ],
      "metadata": {
        "id": "S3VY1gyU81EP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_results = evaluate_test_set(et, X_test_processed, y_test_multilabel, 'Extra Tree')\n",
        "test_results"
      ],
      "metadata": {
        "id": "JMPVINhZH-Ed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_test_set(model, X_test, y_test_multilabel, model_name):\n",
        "    y_pred_proba = model.predict_proba(X_test)\n",
        "\n",
        "    # Top-5 and Top-10\n",
        "    top5_hits = []\n",
        "    top10_hits = []\n",
        "\n",
        "    for i in range(len(y_test_multilabel)):\n",
        "        true_species = set(np.where(y_test_multilabel[i] == 1)[0])\n",
        "        top5_pred = set(np.argsort(y_pred_proba[i])[-5:][::-1])\n",
        "        top10_pred = set(np.argsort(y_pred_proba[i])[-10:][::-1])\n",
        "\n",
        "        top5_hits.append(len(true_species & top5_pred) > 0)\n",
        "        top10_hits.append(len(true_species & top10_pred) > 0)\n",
        "\n",
        "    # AUC-ROC\n",
        "    try:\n",
        "        auc_roc = roc_auc_score(y_test_multilabel, y_pred_proba, average='weighted', multi_class='ovr')\n",
        "    except:\n",
        "        auc_roc = np.nan\n",
        "\n",
        "    # Species Coverage\n",
        "    species_coverage = np.mean(np.sum(y_pred_proba > 0.01, axis=1))\n",
        "\n",
        "    return {\n",
        "        'Model': model_name,\n",
        "        'Top-5': np.mean(top5_hits),\n",
        "        'Top-10': np.mean(top10_hits),\n",
        "        'AUC-ROC': auc_roc,\n",
        "        'Species Coverage': species_coverage\n",
        "    }"
      ],
      "metadata": {
        "id": "R_6HcjVuUvNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_results = evaluate_test_set(lgbm, X_test_processed, y_test_multilabel, 'Light GBM')\n",
        "test_results"
      ],
      "metadata": {
        "id": "pslNG_68TefV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#One vs Rest on the best model"
      ],
      "metadata": {
        "id": "guUykyCSWkdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "# Convert single-label train to multilabel format\n",
        "y_train_lists = [[label] for label in y_train]\n",
        "y_val_lists = [[label] for label in y_val]\n",
        "\n",
        "mlb_train = MultiLabelBinarizer(classes=range(len(le.classes_)))\n",
        "y_train_multilabel = mlb_train.fit_transform(y_train_lists)\n",
        "y_val_multilabel = mlb_train.transform(y_val_lists)"
      ],
      "metadata": {
        "id": "zz8hlFx3WWTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "\n",
        "ovr_base = RandomForestClassifier(\n",
        "    n_estimators=50,\n",
        "    max_depth=5,\n",
        "    class_weight='balanced',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "ovr = MultiOutputClassifier(ovr_base, n_jobs=-1)"
      ],
      "metadata": {
        "id": "_KrQgDytWqO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Training: One-vs-Rest (500 classifiers)\")\n",
        "start = time.time()\n",
        "ovr.fit(X_train, y_train_multilabel)\n",
        "ovr_train_time = time.time() - start\n",
        "print(f\"Training completed in {ovr_train_time:.2f} seconds\")"
      ],
      "metadata": {
        "id": "pv0GjFGMWr3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_val_probs_ovr = np.zeros((len(X_val), len(le.classes_)))\n",
        "for i, estimator in enumerate(ovr.estimators_):\n",
        "    y_val_probs_ovr[:, i] = estimator.predict_proba(X_val)[:, 1]\n",
        "\n",
        "val_results_ovr = evaluate_model(y_val_multilabel, y_val_probs_ovr, is_multilabel=True,\n",
        "                                  model_name=\"One-vs-Rest\", split_name=\"Validation\")"
      ],
      "metadata": {
        "id": "z3nc8OdGWto3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_probs_ovr = np.zeros((len(X_test_processed), len(le.classes_)))\n",
        "for i, estimator in enumerate(ovr.estimators_):\n",
        "    y_test_probs_ovr[:, i] = estimator.predict_proba(X_test_processed)[:, 1]\n",
        "\n",
        "\n",
        "test_results_ovr = evaluate_model(y_test_multilabel, y_test_probs_ovr, is_multilabel=True,\n",
        "                                   model_name=\"One-vs-Rest\", split_name=\"Test\")"
      ],
      "metadata": {
        "id": "J8HjjXoTWvYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def evaluate_ovr(model, X, y_multilabel, split_name=\"Val\"):\n",
        "\n",
        "    print(f\"\\nEvaluating OvR on {split_name} Set...\")\n",
        "\n",
        "    y_pred_proba_list = model.predict_proba(X)\n",
        "    y_pred_proba = np.array([y_pred_proba_list[i][:, 1] for i in range(len(y_pred_proba_list))]).T\n",
        "\n",
        "    # Top-5 and Top-10\n",
        "    top5_hits = []\n",
        "    top10_hits = []\n",
        "\n",
        "    for i in range(len(y_multilabel)):\n",
        "        true_species = set(np.where(y_multilabel[i] == 1)[0])\n",
        "        top5_pred = set(np.argsort(y_pred_proba[i])[-5:][::-1])\n",
        "        top10_pred = set(np.argsort(y_pred_proba[i])[-10:][::-1])\n",
        "\n",
        "        top5_hits.append(len(true_species & top5_pred) > 0)\n",
        "        top10_hits.append(len(true_species & top10_pred) > 0)\n",
        "\n",
        "    top5_acc = np.mean(top5_hits)\n",
        "    top10_acc = np.mean(top10_hits)\n",
        "\n",
        "    # AUC-ROC\n",
        "    try:\n",
        "        auc_roc = roc_auc_score(y_multilabel, y_pred_proba, average='weighted', multi_class='ovr')\n",
        "    except:\n",
        "        auc_roc = np.nan\n",
        "\n",
        "    # Species Coverage\n",
        "    species_coverage = np.mean(np.sum(y_pred_proba > 0.01, axis=1))\n",
        "\n",
        "    print(f\"  Top-5 Accuracy:      {top5_acc:.4f}\")\n",
        "    print(f\"  Top-10 Accuracy:     {top10_acc:.4f}\")\n",
        "    print(f\"  AUC-ROC:             {auc_roc:.4f}\")\n",
        "    print(f\"  Species Coverage:    {species_coverage:.2f}\")\n",
        "\n",
        "    return {\n",
        "        'Model': 'One-vs-Rest',\n",
        "        'Split': split_name,\n",
        "        'Top-5': top5_acc,\n",
        "        'Top-10': top10_acc,\n",
        "        'AUC-ROC': auc_roc,\n",
        "        'Species Coverage': species_coverage\n",
        "    }\n",
        "\n",
        "# Validation\n",
        "ovr_val_results = evaluate_ovr(ovr, X_val, y_val_multilabel, \"Validation\")\n",
        "\n",
        "# Test\n",
        "ovr_test_results = evaluate_ovr(ovr, X_test_processed, y_test_multilabel, \"Test\")\n",
        "\n",
        "ovr_results_df = pd.DataFrame([ovr_val_results, ovr_test_results])\n",
        "\n",
        "print()\n",
        "print(\"ONE-VS-REST RESULTS:\")\n",
        "print(ovr_results_df)"
      ],
      "metadata": {
        "id": "EKGNNK3GUUsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Classifier Chain on the best model"
      ],
      "metadata": {
        "id": "wmHmKLE9WxSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.multioutput import ClassifierChain\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "cc_base = RandomForestClassifier(\n",
        "    n_estimators=50,\n",
        "    max_depth=5,\n",
        "    class_weight='balanced',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "cc = ClassifierChain(cc_base, order='random', random_state=42)"
      ],
      "metadata": {
        "id": "v8h3jeM5Wz0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Training: Classifier Chain\")\n",
        "start = time.time()\n",
        "cc.fit(X_train, y_train_multilabel)\n",
        "cc_train_time = time.time() - start\n",
        "print(f\"Training completed in {cc_train_time:.2f} seconds\")"
      ],
      "metadata": {
        "id": "7M_B0f7MW145"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def evaluate_classifier_chain(model, X, y_multilabel, split_name=\"Val\"):\n",
        "\n",
        "    print(f\"\\nEvaluating Classifier Chain on {split_name} Set...\")\n",
        "\n",
        "    y_pred_proba = model.predict_proba(X)\n",
        "\n",
        "    # If it returns binary predictions instead of probabilities, use predict\n",
        "    if y_pred_proba.max() <= 1 and y_pred_proba.min() >= 0:\n",
        "        pass\n",
        "    else:\n",
        "        y_pred_proba = model.predict(X).astype(float)\n",
        "\n",
        "    # Top-5 and Top-10\n",
        "    top5_hits = []\n",
        "    top10_hits = []\n",
        "\n",
        "    for i in range(len(y_multilabel)):\n",
        "        true_species = set(np.where(y_multilabel[i] == 1)[0])\n",
        "        top5_pred = set(np.argsort(y_pred_proba[i])[-5:][::-1])\n",
        "        top10_pred = set(np.argsort(y_pred_proba[i])[-10:][::-1])\n",
        "\n",
        "        top5_hits.append(len(true_species & top5_pred) > 0)\n",
        "        top10_hits.append(len(true_species & top10_pred) > 0)\n",
        "\n",
        "    top5_acc = np.mean(top5_hits)\n",
        "    top10_acc = np.mean(top10_hits)\n",
        "\n",
        "    # AUC-ROC\n",
        "    try:\n",
        "        auc_roc = roc_auc_score(y_multilabel, y_pred_proba, average='weighted', multi_class='ovr')\n",
        "    except:\n",
        "        auc_roc = np.nan\n",
        "\n",
        "    # Species Coverage\n",
        "    species_coverage = np.mean(np.sum(y_pred_proba > 0.01, axis=1))\n",
        "\n",
        "    print(f\"  Top-5 Accuracy:      {top5_acc:.4f}\")\n",
        "    print(f\"  Top-10 Accuracy:     {top10_acc:.4f}\")\n",
        "    print(f\"  AUC-ROC:             {auc_roc:.4f}\")\n",
        "    print(f\"  Species Coverage:    {species_coverage:.2f}\")\n",
        "\n",
        "    return {\n",
        "        'Model': 'Classifier Chain',\n",
        "        'Split': split_name,\n",
        "        'Top-5': top5_acc,\n",
        "        'Top-10': top10_acc,\n",
        "        'AUC-ROC': auc_roc,\n",
        "        'Species Coverage': species_coverage\n",
        "    }\n",
        "\n",
        "# Validation\n",
        "cc_val_results = evaluate_classifier_chain(cc, X_val, y_val_multilabel, \"Validation\")\n",
        "\n",
        "# Test\n",
        "cc_test_results = evaluate_classifier_chain(cc, X_test_processed, y_test_multilabel, \"Test\")\n",
        "\n",
        "cc_results_df = pd.DataFrame([cc_val_results, cc_test_results])\n",
        "\n",
        "print()\n",
        "print(\"CLASSIFIER CHAIN RESULTS:\")\n",
        "print(cc_results_df)"
      ],
      "metadata": {
        "id": "fl44fHvw2OqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_val_probs_cc = cc.predict_proba(X_val)\n",
        "\n",
        "val_results_cc = evaluate_model(y_val_multilabel, y_val_probs_cc, is_multilabel=True,\n",
        "                                 model_name=\"Classifier Chain\", split_name=\"Validation\")"
      ],
      "metadata": {
        "id": "QQ7JWu8CW3l1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_probs_cc = cc.predict_proba(X_test_processed)\n",
        "test_results_cc = evaluate_model(y_test_multilabel, y_test_probs_cc, is_multilabel=True,\n",
        "                                  model_name=\"Classifier Chain\", split_name=\"Test\")"
      ],
      "metadata": {
        "id": "HDrdS_AZW5Ud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Error and Performance Analysis"
      ],
      "metadata": {
        "id": "ofQisq5WOeIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "nDXRN6DKOhvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest"
      ],
      "metadata": {
        "id": "GhEGSuAEPexA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=5,\n",
        "    min_samples_split=10,\n",
        "    class_weight=class_weights,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf, rf_time, rf_mem = train_and_evaluate(rf, \"Random Forest\", X_train, y_train)"
      ],
      "metadata": {
        "id": "KA_ES-6vPexB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generating predictions from Random Forest on test set\")\n",
        "y_test_pred_proba = rf.predict_proba(X_test_processed)\n",
        "y_test_pred = rf.predict(X_test_processed)\n",
        "\n",
        "print(f\"Test predictions shape: {y_test_pred_proba.shape}\")"
      ],
      "metadata": {
        "id": "CWbRBM6DOiyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All Species Performance Analysis"
      ],
      "metadata": {
        "id": "gSoUX-nO_E-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "species_freq = Counter(y_train)\n",
        "\n",
        "species_performance = []\n",
        "\n",
        "for species_id in range(500):\n",
        "    # Get samples where this species is present in test set\n",
        "    species_mask = y_test_multilabel[:, species_id] == 1\n",
        "    n_samples = species_mask.sum()\n",
        "\n",
        "    if n_samples == 0:\n",
        "        continue\n",
        "\n",
        "    # Check if species appears in top-5 predictions\n",
        "    top5_preds = np.argsort(y_test_pred_proba, axis=1)[:, -5:]\n",
        "    species_in_top5 = np.any(top5_preds == species_id, axis=1)\n",
        "\n",
        "    top5_recall = np.mean(species_in_top5[species_mask])\n",
        "\n",
        "    # Average probability assigned to this species\n",
        "    avg_prob = np.mean(y_test_pred_proba[species_mask, species_id])\n",
        "\n",
        "    species_performance.append({\n",
        "        'species_id': species_id,\n",
        "        'train_frequency': species_freq.get(species_id, 0),\n",
        "        'test_samples': int(n_samples),\n",
        "        'top5_recall': top5_recall,\n",
        "        'avg_probability': avg_prob\n",
        "    })\n",
        "\n",
        "species_perf_df = pd.DataFrame(species_performance)\n",
        "\n",
        "# Categorize by frequency\n",
        "species_perf_df['freq_category'] = pd.cut(\n",
        "    species_perf_df['train_frequency'],\n",
        "    bins=[0, 250, 750, 3000],\n",
        "    labels=['Rare (<250)', 'Medium (250-750)', 'Frequent (>750)']\n",
        ")\n",
        "\n",
        "print(f\"\\nTotal species evaluated: {len(species_perf_df)}\")\n",
        "print(f\"\\nPerformance by frequency category:\")\n",
        "print(species_perf_df.groupby('freq_category')['top5_recall'].agg(['mean', 'std', 'count']))"
      ],
      "metadata": {
        "id": "E9CjcH3UOi1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Category wise plot\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "species_perf_df.boxplot(column='top5_recall', by='freq_category',\n",
        "                         patch_artist=True, figsize=(10, 7))\n",
        "plt.suptitle('')\n",
        "plt.title('Top-5 Recall by Species Frequency Category', fontsize=15, fontweight='bold')\n",
        "plt.xlabel('Species Frequency Category', fontsize=13, fontweight='bold')\n",
        "plt.ylabel('Top-5 Recall', fontsize=13, fontweight='bold')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('performance_by_category.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RisoYpHxOnZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most Confused Pairs"
      ],
      "metadata": {
        "id": "GkbMqfHf_pjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find top-1 predictions for each test sample\n",
        "top1_preds = np.argmax(y_test_pred_proba, axis=1)\n",
        "\n",
        "# Build confusion matrix (only the top confusions)\n",
        "confusion_pairs = []\n",
        "\n",
        "for i in range(len(y_test_multilabel)):\n",
        "    true_species = np.where(y_test_multilabel[i] == 1)[0]\n",
        "    pred_species = top1_preds[i]\n",
        "\n",
        "    # If prediction is wrong\n",
        "    if pred_species not in true_species:\n",
        "        for true_sp in true_species:\n",
        "            confusion_pairs.append({\n",
        "                'true_species': true_sp,\n",
        "                'predicted_species': pred_species,\n",
        "                'true_freq': species_freq.get(true_sp, 0),\n",
        "                'pred_freq': species_freq.get(pred_species, 0)\n",
        "            })\n",
        "\n",
        "confusion_df = pd.DataFrame(confusion_pairs)\n",
        "\n",
        "if len(confusion_df) > 0:\n",
        "    confusion_counts = confusion_df.groupby(['true_species', 'predicted_species']).size()\n",
        "    top_confusions = confusion_counts.nlargest(15)\n",
        "\n",
        "    print(f\"\\nTop 15 Most Confused Species Pairs:\")\n",
        "    for (true_sp, pred_sp), count in top_confusions.items():\n",
        "        print(f\"True: {true_sp:3d} Predicted as: {pred_sp:3d} | Count: {count:3d}\")\n",
        "\n",
        "    # Create subset confusion matrix for top confused species\n",
        "    top_confused_species = set()\n",
        "    for (true_sp, pred_sp), _ in top_confusions.head(20).items():\n",
        "        top_confused_species.add(true_sp)\n",
        "        top_confused_species.add(pred_sp)\n",
        "\n",
        "    top_confused_species = sorted(list(top_confused_species))[:20]\n",
        "\n",
        "    # Build confusion matrix subset\n",
        "    conf_matrix = np.zeros((len(top_confused_species), len(top_confused_species)))\n",
        "\n",
        "    for (true_sp, pred_sp), count in confusion_counts.items():\n",
        "        if true_sp in top_confused_species and pred_sp in top_confused_species:\n",
        "            true_idx = top_confused_species.index(true_sp)\n",
        "            pred_idx = top_confused_species.index(pred_sp)\n",
        "            conf_matrix[true_idx, pred_idx] = count\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(conf_matrix, annot=False, cmap='YlOrRd',\n",
        "                xticklabels=top_confused_species,\n",
        "                yticklabels=top_confused_species,\n",
        "                cbar_kws={'label': 'Confusion Count'})\n",
        "    plt.xlabel('Predicted Species', fontsize=13, fontweight='bold')\n",
        "    plt.ylabel('True Species', fontsize=13, fontweight='bold')\n",
        "    plt.title('Confusion Matrix - Top 20 Most Confused Species', fontsize=15, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('confusion_heatmap.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "b67r-H8SOoa5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Importance"
      ],
      "metadata": {
        "id": "UDXD36fWA3wb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_importance = rf.feature_importances_\n",
        "\n",
        "feature_names = (numeric_features + ['climate_PC1', 'climate_PC2'] +\n",
        "                 list(preprocessor.named_transformers_['cat'].get_feature_names_out()))\n",
        "\n",
        "importance_df = pd.DataFrame({\n",
        "    'feature': feature_names,\n",
        "    'importance': feature_importance\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(f\"\\nTop 20 Most Important Features:\")\n",
        "print(importance_df.head(20))"
      ],
      "metadata": {
        "id": "GoqFNvozOryF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "top_features = importance_df.head(20)\n",
        "plt.barh(range(len(top_features)), top_features['importance'],\n",
        "         color='blue', edgecolor='black', linewidth=1)\n",
        "plt.yticks(range(len(top_features)), top_features['feature'])\n",
        "plt.xlabel('Importance', fontsize=13, fontweight='bold')\n",
        "plt.ylabel('Feature', fontsize=13, fontweight='bold')\n",
        "plt.title('Top 20 Most Important Features - Random Forest', fontsize=15, fontweight='bold')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.grid(axis='x', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vJfMeZ__OtP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other Analysis"
      ],
      "metadata": {
        "id": "D7eQYit86vKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_probs = np.max(y_test_pred_proba, axis=1)\n",
        "\n",
        "species_coverage = np.sum(y_test_pred_proba > 0.01, axis=1)\n",
        "\n",
        "correct_predictions = []\n",
        "for i in range(len(y_test_multilabel)):\n",
        "    true_species = set(np.where(y_test_multilabel[i] == 1)[0])\n",
        "    top5_pred = set(np.argsort(y_test_pred_proba[i])[-5:][::-1])\n",
        "    correct_predictions.append(len(true_species & top5_pred) > 0)\n",
        "\n",
        "correct_predictions = np.array(correct_predictions)\n",
        "\n",
        "print(f\"\\nConfidence Statistics:\")\n",
        "print(f\"Mean max probability: {max_probs.mean():.4f}\")\n",
        "print(f\"Mean species coverage: {species_coverage.mean():.2f}\")\n",
        "print(f\"Correct predictions: {correct_predictions.sum()} / {len(correct_predictions)}\")\n"
      ],
      "metadata": {
        "id": "EoqTdZHhOxEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "worst_species = species_perf_df.nsmallest(20, 'top5_recall')\n",
        "\n",
        "print(\"\\nTop 20 Worst Performing Species:\")\n",
        "print(worst_species[['species_id', 'train_frequency', 'test_samples', 'top5_recall', 'avg_probability']])"
      ],
      "metadata": {
        "id": "BQfsZhT4O00H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confidence_bins = pd.cut(max_probs, bins=[0, 0.1, 0.2, 0.3, 0.5, 1.0],\n",
        "                         labels=['0-0.1', '0.1-0.2', '0.2-0.3', '0.3-0.5', '0.5-1.0'])\n",
        "\n",
        "accuracy_by_confidence = []\n",
        "for bin_label in ['0-0.1', '0.1-0.2', '0.2-0.3', '0.3-0.5', '0.5-1.0']:\n",
        "    mask = confidence_bins == bin_label\n",
        "    if mask.sum() > 0:\n",
        "        acc = correct_predictions[mask].mean()\n",
        "        count = mask.sum()\n",
        "        accuracy_by_confidence.append({\n",
        "            'confidence_bin': bin_label,\n",
        "            'accuracy': acc,\n",
        "            'count': count\n",
        "        })\n",
        "\n",
        "acc_conf_df = pd.DataFrame(accuracy_by_confidence)\n",
        "print(\"\\nAccuracy by Confidence Level:\")\n",
        "print(acc_conf_df)"
      ],
      "metadata": {
        "id": "2eiLVzFEO4E_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}